# Reproducible Vibes in Research

### Summary of our work

This project is centred around the use of large language models (LLMs) for coding tasks, typically called AI-assisted coding or, more informally, “vibe coding”. There are two main components of our project:

- Updating and extending an existing Carpentry course on this same topic: link.
    
- Developing a proposal for a TDCC NES open call.
    

As part of the retreat team work, we followed 3 distinct phases for each of the working sessions.

First, we brainstormed about possible ways we could move this project forward (both on the Carpentries course and the TDCC NES proposal fronts), such as topics to cover within the course and to tackle in the proposal..

Then, we refined our ideas and re-evaluated which items made sense and which ones didn’t really fit or were not worth the time. We discussed strategies for achieving a successful Carpentries course and project proposal. We also initiated a conversation with a different but related retreat team (the _research software training_ team) about how our projects could potentially link with each other.

Lastly, we collected our ideas (scattered across paper notes, post-its, and digital documents) into a GitHub repository to capture the progress we’ve made so far and have it in a shareable state, ready for further collaboration beyond the retreat.

_Contributors_

- Santosh Ilamparuthi
    
- Olga Minaeva
    
- Errol Neo
    
- Julian Lopez Gordillo
    

Problem Statement - Why now?

LLMs are already widely available, they are already integrated in many domains, including daily life. LLM in coding offers a lot of benefits (...) but at the same time there is a lot of drawbacks. Users often use it incorrectly, misusing the tool, making unsafe choices (elaborate more). To prevent it, we want to implement safe use of LLMs in the general coding practices. We don’t want to explicitly encourage people to use the AI tools rather to give them the necessary skills and knowledge to do it safely if/when they choose to use them.

Important to explain how it is related to NES, not just a general problem that needs to be fixed. Why is it a NES problem? In many NES disciplines you’re introduced to coding early on or even expected to code in your daily work, so the pressure to code efficiently is high, especially for early career researchers. This leads to some researchers adopting

### Roadmap

Challenges:

1. FAIR data
    
2. Sustainable software and eScience —> focus on more general sustainable coding and skills needed for it (how to use the AI tools and the ability to figure out which tasks to use these tools for)
    
3. Connection to the international activities —> The Carpentries
    
4. Long-term data archiving
    
5. Locate computing capacity close to the storage
    
6. Human capital
    
7. Cross-field collaboration
    
8. Use specific code editor? Or code editor-agnostic?
    

## Notes and Brainstorming

- Work on this topic could take a few forms and be published on different places: as a Carpentry course, but also as a higher-level page on _The Turing Way_, or within the documentation of Digital Competence Centres or other institutional resources...
    
- As an output from our work during the retreat, we aim to prepare a GitHub repository collecting our ideas and progress to enable further collaborations beyond the retreat.
    

#### _Carpentries course_

- **Main problem:** People that have no specific training in software still need to write some code here and there anyway. However, usually that happens very inefficiently, and they might end up struggling a lot and hating coding because of that.
    
- **Solution:** LLM-based code assistants can help with this. Researchers are going to use these tools anyway, so we might as well teach them about the tool, how to use it, what they should and shouldn’t use it for, etc.
    
- **Topics to address:**
    
- Ethics about the usage of LLMs
    
- Sustainability concerns (huge energy consumption for relatively simple tasks that could be achieved without them).
    
- Recognition and attribution of authorship when the work includes a (potential large) amount of LLM-generated content.
    
- Identification, citation and acknowledgements for LLM-generated code: it is currently very difficult (if not impossible) to keep track of which contributions were made by an AI; a mechanism for being able to share them (ideally, at least in certain cases, also to assign them a persistent identifier) and even add them to the reference list.
    
- Some of these topics are very deep and would make the course exceedingly lengthy. We should mention that they may constitute an ethical issue, while also giving a broader view of the landscape. We should do this as a word of warning, not with the intent of providing a comprehensive answer to those issues.
    
- For example: shifting responsibilities for carbon emissions to the end-users has been a widespread tactic from big corporations, whose ability to cut emissions in their industrial processes is several orders of magnitude higher than that of the average citizen. We should acknowledge that both of this perspectives exist, and let the students dive more into the topic elsewhere.
    
- Security concerns regarding the management of the data and information that the LLM has access to. This is especially relevant when dealing with personal and sensitive data.
    
- Locally self-hosted LLMs: this directly addresses the previous point about security, among other aspects.
    
- It is definitely a must, given our context in open science. Furthermore, this links with other similar areas such as _digital sovereignty,_ that are currently very relevant in the European policy domain.
    
- A distinction should be made about the different degrees of openness of LLMs: from completely closed or proprietary, to open-weights, or fully open-source (disclosing also which training strategy and datasets were used).
    
- Some universities and institutions are already trying to self-host LLMs in the Netherlands. There is a proposal for open source LLMs hosted at SURF.
    
- Usage examples
    
- Do’s:
    
- Mechanical and repetitive tasks such as refactoring, generating templates, structuring directories and file organisations, boilerplate code and base documentation.
    
- Debugging, code reviewing, “pair-programming”.
    
- Translating code between different languages.
    
- Don’ts:
    
- Generation of large chunks of code.
    
- Accepting code without review or testing.
    
- Including sensitive data or information.
    
- Not checking the institutional policy on the use of AI.
    
- Difference between LLM code assistants, and other tools such as code linters, formatters, debuggers, etc. Also between AI-enabled code editors, the AI models themselves, and even an overview of different families of LLMs and their different versions (and how this development-deployment cycle usually works).
    

#### _TDCC NES proposal_

- We need to make very clear who our target group/audience is, how our proposed developments would impact them, and why that would be beneficial. Simultaneously, we also need to draw clear boundaries for which areas we are not going to tackle, either because they are out of scope, or because they might be counterproductive.
    
- Addressing the problem of sustainable software:
    
- Due to how easily LLMs can be accessed, people are using them for all sorts of tasks before even evaluating other options, and without consulting relevant resources about best practices in software development (as well as data stewards and DCCs, etc). They are, therefore, skipping an important skill that they should learn and should be part of their toolkit.
    
- As part of the project, we propose to teach people how to use LLM-assisted coding properly: what they should use it for, what they shouldn’t, what are the limits of these tools, and how they should be complemented with other skills and tools.
    
- For example, LLM can be used to generate templates and placeholders following good software practices, such as a base code repository with a well-structured README, a license, a CONTRIBUTING.md file, a coherent directory for the scripts, etc.
    
- Or, as part of the course, students can learn how to check the resulting code of LLM-interactions against the best practices and requirements by their DCCs.
    
- Difference with existing resources
    
- Most universities have some existing guidelines on whether use of AI is allowed, when is it okay, and warnings about ethical considerations. However, everything is kept at a very general level, and no practical guidelines are provided on how to actually leverage LLMs.
    
- Here, we propose a practical course where participants follow instructions on how to use LLMs for coding. The course is therefore hands-on, and the skills and insights taken from the course are actionable for their academic career.
    
- Why NES
    
- While the availability of LLMs is transversal across disciplines, and therefore, also occurs in other communities apart from NES, it is in NES where this problem is the most acute. Most researchers within NES disciplines have already a basic coding training, and therefore, are more empowered to (mis)use LLM-assisted coding.
    
- Consequently, it is the community where most code is generated, and therefore, where the potential influence of LLMs—for good or bad—is the greatest.
    
- On one hand, a community were basic coding skills are widespread is a requirement for this, of which NES is the best candidate. Conversely, providing the NES community with good practices for using LLMs can therefore have the most impact as compared to others. Thirdly, as it may happen in other technical topics, the NES community can be the model for how things should be done, and other less technically-focused communities will follow suit.
